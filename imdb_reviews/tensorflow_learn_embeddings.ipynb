{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Credit: Coursera\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Flatten, Embedding\n",
    "from keras.models import Sequential\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n",
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "train_sentences = []\n",
    "train_labels = []\n",
    "test_sentences = []\n",
    "test_labels = []\n",
    "\n",
    "for s,l in train_data:\n",
    "    train_sentences.append(str(s.numpy()))\n",
    "    train_labels.append(l.numpy())\n",
    "    \n",
    "for s,l in test_data:\n",
    "    test_sentences.append(str(s.numpy()))\n",
    "    test_labels.append(l.numpy())\n",
    "    \n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "out_of_vocab = '<OOV>'\n",
    "truncate_mode = 'post'\n",
    "max_len = 130\n",
    "embedding_dim = 16\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=out_of_vocab)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_len)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 130, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2080)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 6)                 12486     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 172,493\n",
      "Trainable params: 172,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 4s 156us/step - loss: 5.9716e-05 - accuracy: 1.0000 - val_loss: 0.7720 - val_accuracy: 0.8483\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 4s 156us/step - loss: 3.7601e-05 - accuracy: 1.0000 - val_loss: 0.8025 - val_accuracy: 0.8482\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 4s 157us/step - loss: 2.3879e-05 - accuracy: 1.0000 - val_loss: 0.8303 - val_accuracy: 0.8479\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 4s 156us/step - loss: 1.5301e-05 - accuracy: 1.0000 - val_loss: 0.8601 - val_accuracy: 0.8480\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 4s 154us/step - loss: 9.8048e-06 - accuracy: 1.0000 - val_loss: 0.8880 - val_accuracy: 0.8477\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 4s 159us/step - loss: 6.2513e-06 - accuracy: 1.0000 - val_loss: 0.9186 - val_accuracy: 0.8479\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 4s 155us/step - loss: 4.0651e-06 - accuracy: 1.0000 - val_loss: 0.9483 - val_accuracy: 0.8477\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 2.6137e-06 - accuracy: 1.0000 - val_loss: 0.9790 - val_accuracy: 0.8470\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 1.6824e-06 - accuracy: 1.0000 - val_loss: 1.0054 - val_accuracy: 0.8476\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 4s 157us/step - loss: 1.1023e-06 - accuracy: 1.0000 - val_loss: 1.0351 - val_accuracy: 0.8475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2221fc7ca48>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences, train_labels, epochs=10, validation_data=(test_sequences, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w')\n",
    "out_m = io.open('meta.tsv', 'w')\n",
    "\n",
    "reverse_word_index = dict([(value, word) for (word, value) in word_index.items()])\n",
    "\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_v.write(word + '\\n')\n",
    "    out_m.write('\\t'.join([str(x) for x in embeddings]) + '\\n')\n",
    "\n",
    "out_m.close()\n",
    "out_v.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
